---
title: "Unit 3 Homework"
author: "Answer Key"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Best Game in the Casino

You flip a fair coin 3 times, and get a different amount of money depending on how many heads you get. For 0 heads, you get $0. For 1 head, you get $2. For 2 heads, you get $4. Your expected winnings from the game are $6.

(a) How much do you get paid if the coin comes up heads 3 times?

Let random variable H represent the number of heads.  Let X(H) represent the winnings.  Since X is a function of a random variable, it is also a random variable.

E(X) = 6 

P(H = 0) = 1/8

P(H = 1) = 3/8

P(H = 2) = 3/8

P(H = 3) = 1/8

E(X) = (P(H = 0) * 0) + (P(H = 1) * 2) + (P(H = 2) * 4) + (P(H = 3) * x)

6 = (1/8 * 0) + (3/8 * 2) + (3/8 * 4) + (1/8 * x)

6 = $0 + 6/8 + 12/8 + 1/8x

24/4 = 9/4 + 1/8x

15/4 = 1/8x

120/4 = x

x = $30

(b) Write down a complete expression for the cumulative probability function for your winnings from the game.

$F(x) = \begin{cases}
0, &x < 0 \\
1/8,  &0 \leq x < 2 \\
1/2, &2 \leq x < 4 \\
7/8, &4 \leq x < 30 \\
1, &x \geq 30\\
\end{cases}$

# 2. Reciprocal Dice

Let $X$ be a random variable representing the outcome of rolling a 6-sided die.  Before the die is rolled, you are given two options:

(a) You get $1/E(X)$ in dollars right away.

(b) You wait until the die is rolled, then get $1/X$ in dollars.

Which option is better for you, in expectation?

*Solution:*

We know that $X$ can take on any integer value in the interval $[1,6]$ with equal probability, so its p.m.f. is $p(X=x) = \frac{1}{6}$, for all x.

To evaluate the first option, let us calculate $E(X)$:

$$E(X) = \sum_x xp(x) = 1\cdot\frac{1}{6} + 2\cdot\frac{1}{6} + 3\cdot\frac{1}{6} + 4\cdot\frac{1}{6} + 5\cdot\frac{1}{6} + 6\cdot\frac{1}{6} = \frac{7}{2}$$

Therefore, $\frac{1}{E(X)} = \frac{2}{7} = 0.29$.

To evaluate the second option, let us calculate $E(\frac{1}{X})$:

Let us define $g(x) = \frac{1}{x}$.

$$E(g(x)) = \sum_x g(x)p(x) = \sum_x \frac{1}{x}\cdot\frac{1}{6} = \frac{1}{6}\sum_x \frac{1}{x} = \frac{1}{6} (1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \frac{1}{6}) = \frac{49}{120}$$

Therefore, $E(g(x)) = E(\frac{1}{X}) = \frac{49}{120} = 0.41$.

In this case, $E(\frac{1}{X}) > \frac{1}{E(X)}$, so in expectation, option (b) is better!


# 3. The Baseline for Measuring Deviations

Given any random variable $X$ and a real number $t$, we can define another random variable $Y = (X - t)^2$. In other words, for any random variable $X$, we can choose a real number, $t$, as a baseline and calculate the squared deviation of $X$ away from $t$.

You might wonder why we often square deviations (instead of taking an absolute value, or cubing them, etc.).  This exercise will shed some light on why this is a natural choice.

(a) Write down an expression for $E(Y)$ and simplify it as much as you can.  Even though we haven't proved this yet, you can use the fact that for any two random variables, $A$ and $B$, $E(A + B) = E(A) + E(B)$.

(b) Taking a partial derivative with respect to $t$, compute the value of $t$ that minimizes $E(Y)$.  (Hint: Your answer should be a very familiar value)

(c) What is the value of $E(Y)$ for this choice of $t$?
(Hint: this should also be a very familiar value)

*Solution:*

(a) $$E(Y) = E((X-t)^2) = E(X^2 - 2Xt + t^2) = E(X^2) + E(-2Xt) + E(t^2)$$
$$E(Y) = E(X^2) - 2tE(X) + t^2$$

(b) $$\frac{\partial E(Y)}{\partial t} = 0 - 2E(X) + 2t = 2(t - E(X))$$

To find the minimum, set $0 = \frac{\partial E(Y)}{\partial t} = 2(t-E(X))$, and we get:

$$t = E(x)$$
It makes sense that the value of t that minimizes E(Y) will be E(X) because all the deviations of the random variable X are centered around this expectation.

(c) Plugging this value for $t$ back into $E(Y) = E(X^2) - 2tE(X) + t^2$, we get:

$$E(Y) = E(X^2) - 2E(X)E(X) + [E(X)]^2 = E(X^2) - 2[E(X)]^2 + [E(X)]^2$$

Therefore, when $Y = (X-t)^2$ and $t = E(X)$,

$$E(Y) = E(X^2) - [E(X)]^2$$

This is the variance of $X$!


<!--
# The Baseline for Measuring Deviations

Given any random variable X and a real number t, we can define another random variable Y=(X-t). In other words, for any random variable, we can choose a real number, t, as a baseline and calculate the squared deviation of X from t. You might wonder why we often use square deviations (instead of taking an absolute value, or cubing them, etc.). This exercise will shed some light on why this is a natural choice.

(a) Write down an expression for E[Y] and use properties of expectation to simplify it as much as you can.

$E[Y] = E[(X-t)^2] = E[X^2 - 2tX + t^2] = E[X^2]-2tE[X]+t^2$


(b) Taking a partial derivative with respect to t, compute the value of t that minimizes E[Y]. (Hint: Your answer should be a very familiar value)

$\frac{\partial}{\partial t} E[Y] = \frac{\partial}{\partial t}\big[E[X^2]-2tE[X]+t^2 \big] = -2E[X] + 2t$

Setting this equal to zero (our first-order condition), we get $t = E(X)$.  This shouldn't be too surprising: The value of t that minimizes E(Y) will be E(X) because all the deviations of the random variable X are centered around this expectation.

(c) What is the value of E[Y] for this choice of t?

E(Y) = $E(X^{2})-E(X)^{2} = var(X)$, the variance of X
-->

# 4. Optional Advanced Exercise: Heavy Tails
One reason to study the mathematical foundation of statistics is to recognize situations where common intuition can break down.  An unusual class of distributions are those we call *heavy-tailed*.  The exact definition varies, but we'll say that a heavy-tailed distribution is one for which not all moments are finite.  Consider a random variable $M$ with the following pmf:

$$p_M(x) = \begin{cases}
c/x^3, &x \in \{1,2,3,...\}\\
0, &otherwise.\\
\end{cases}
$$

where $c$ is a constant (you can calculate its value if you like, but it's not important).


a) Is $E(M)$ finite?
b) Is $var(M)$ finite?


Heavy-tailed distributions may seem odd, but they're not as rare as you might suspect.  Researchers argue that the distribution of wealth is heavy-tailed; so is the distribution of computer file sizes, insurance payouts, and area burned by forest fires.  These random variables are problematic in that a lot of common statistical techniques don't work on them.  For this class, we'll assume that all of our variables don't have heavy-tails.

*Solution:*

a) We note that x takes on integer values and although the set is infinite , we are in a discrete setting . 
To judge wether $E(M)$ is finite , we apply the definition of expectation:

$E(M) = \sum_{x = 1}^{\infty} x*c/x^3 = \sum_{x = 1}^{\infty} c/x^2=c\sum_{x = 1}^{\infty}1/x^2$

There are a few ways to go about this summation. The key is to refer back to infinite series, convergence, and divergence (typically taught along side integral calculus). One can simply recognize this as a classic *p-series* with $p=2$ and know that it converges and hence expectation is finite.

For the sake of some calculus review, suppose we didn't have the fact handy but at least knew that we could apply different tests of convergence. 

The most readily available one is the *Integral Test*. The rough intuition is that if we can compare this discrete summation to a smooth integral that acts as an overestimate of the summation (going from adding rectangles to a smoother proccess) and the integral itself has a finite result, then the summation (which is smaller) will also converge and produce a finite result. *Note* that all we care to answer is wether the summation is finite, not an exact result.

The Integral Test has a few requirements we need to satisfy before applying:

1) We need a function such that the series terms $a_x =f(x)$ for some $f$. That is, the function matches the series terms on the integer set. 

A natural choice is $1/x^2$ (we don't care about the constant as it won't affect wether we have convergence).

2) $f(x)$ needs to be positive and continuous from 1 to $\infty$ .
We have no discontinuities (note that 0 isn't part of our set) and the function is in fact positive (no zeros, no possibilities in change of sign).

3) $f(x)$ needs to be a decreasing function 
We know that the derivative $-2/x^3 < 0$ from 1 to $\infty$ 

Since we pass the criterion , we can apply the integral test. Note that since we are integrating to $\infty$ , we have an *improper integral* and technically evaluate up to some $t$ and take the limit.
$\lim_{t \to \infty} \int_{1}^{t} 1/x^2 \; dx = lim_{t \to \infty}-1/t + 1 = 1$ 

Hence the integral converges and by the Integral Test, the series also converges. 

That is to say, yes $E(M)$ is finite!

b) There are a number of ways to go about this question, from definition of variance and so on. At the end of the day, we are still wondering if whatever series we look at is convergent or divergent. 

To make use of previous work from part a), we invoke the fact that $var(M)=E(M^2)-E(M)^2$. From part a), we know that $E(M)^2$ is finite (as long as there's comfort in accepting that the square is also finite). So we are left to see if $E(M^2)$ is finite.

We are left to look at the following summation: 

$E(M^2) = \sum_{x = 1}^{\infty} x^2*c/x^3 = c\sum_{x = 1}^{\infty}1/x$

This is a special case where $p=1$ called a *Harmonic Series* that is known to diverge. The proof for this is not within the scope of this course but we encourage those who are curious to look it up as it involves a few elegant moves involving looking at partial series and re-arranging terms. 

Hence $var(M)$ is not finite! To get a rough idea of why this might be, consider trying to draw the function for some c and for a few points. For integer values we have non-zero points but then we suddenly drop to 0 for all real number in betweens ; this sharp fluctuation in some sense, helps give (hand-wavey) credibility to at least a non-constant variance.
